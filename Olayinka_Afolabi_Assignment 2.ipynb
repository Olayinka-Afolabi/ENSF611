{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yellowbrick.datasets.loaders import load_spam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (4600, 57)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y: (4600,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "X, y = load_spam()\n",
    "# TO DO: Print size and type of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4600 entries, 0 to 4599\n",
      "Data columns (total 57 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   word_freq_make              4600 non-null   float64\n",
      " 1   word_freq_address           4600 non-null   float64\n",
      " 2   word_freq_all               4600 non-null   float64\n",
      " 3   word_freq_3d                4600 non-null   float64\n",
      " 4   word_freq_our               4600 non-null   float64\n",
      " 5   word_freq_over              4600 non-null   float64\n",
      " 6   word_freq_remove            4600 non-null   float64\n",
      " 7   word_freq_internet          4600 non-null   float64\n",
      " 8   word_freq_order             4600 non-null   float64\n",
      " 9   word_freq_mail              4600 non-null   float64\n",
      " 10  word_freq_receive           4600 non-null   float64\n",
      " 11  word_freq_will              4600 non-null   float64\n",
      " 12  word_freq_people            4600 non-null   float64\n",
      " 13  word_freq_report            4600 non-null   float64\n",
      " 14  word_freq_addresses         4600 non-null   float64\n",
      " 15  word_freq_free              4600 non-null   float64\n",
      " 16  word_freq_business          4600 non-null   float64\n",
      " 17  word_freq_email             4600 non-null   float64\n",
      " 18  word_freq_you               4600 non-null   float64\n",
      " 19  word_freq_credit            4600 non-null   float64\n",
      " 20  word_freq_your              4600 non-null   float64\n",
      " 21  word_freq_font              4600 non-null   float64\n",
      " 22  word_freq_000               4600 non-null   float64\n",
      " 23  word_freq_money             4600 non-null   float64\n",
      " 24  word_freq_hp                4600 non-null   float64\n",
      " 25  word_freq_hpl               4600 non-null   float64\n",
      " 26  word_freq_george            4600 non-null   float64\n",
      " 27  word_freq_650               4600 non-null   float64\n",
      " 28  word_freq_lab               4600 non-null   float64\n",
      " 29  word_freq_labs              4600 non-null   float64\n",
      " 30  word_freq_telnet            4600 non-null   float64\n",
      " 31  word_freq_857               4600 non-null   float64\n",
      " 32  word_freq_data              4600 non-null   float64\n",
      " 33  word_freq_415               4600 non-null   float64\n",
      " 34  word_freq_85                4600 non-null   float64\n",
      " 35  word_freq_technology        4600 non-null   float64\n",
      " 36  word_freq_1999              4600 non-null   float64\n",
      " 37  word_freq_parts             4600 non-null   float64\n",
      " 38  word_freq_pm                4600 non-null   float64\n",
      " 39  word_freq_direct            4600 non-null   float64\n",
      " 40  word_freq_cs                4600 non-null   float64\n",
      " 41  word_freq_meeting           4600 non-null   float64\n",
      " 42  word_freq_original          4600 non-null   float64\n",
      " 43  word_freq_project           4600 non-null   float64\n",
      " 44  word_freq_re                4600 non-null   float64\n",
      " 45  word_freq_edu               4600 non-null   float64\n",
      " 46  word_freq_table             4600 non-null   float64\n",
      " 47  word_freq_conference        4600 non-null   float64\n",
      " 48  char_freq_;                 4600 non-null   float64\n",
      " 49  char_freq_(                 4600 non-null   float64\n",
      " 50  char_freq_[                 4600 non-null   float64\n",
      " 51  char_freq_!                 4600 non-null   float64\n",
      " 52  char_freq_$                 4600 non-null   float64\n",
      " 53  char_freq_#                 4600 non-null   float64\n",
      " 54  capital_run_length_average  4600 non-null   float64\n",
      " 55  capital_run_length_longest  4600 non-null   int64  \n",
      " 56  capital_run_length_total    4600 non-null   int64  \n",
      "dtypes: float64(55), int64(2)\n",
      "memory usage: 2.0 MB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 4600 entries, 0 to 4599\n",
      "Series name: is_spam\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "4600 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 36.1 KB\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "# I need to get the summary of the dataset. This gives an overview of Non-Null count\n",
    "# There are 57 columns and 4600 rowa\n",
    "X.info()\n",
    "y.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fcae7659-e1d4-420e-abdd-6dd5536bfe5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a new feature matrix named X_small and target vector names y_small that contains 5% of the data\n",
    "\n",
    "X_small_train, X_small_test, y_small_train, y_small_test, = train_test_split(X, y, test_size = 0.95, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e118ab2e-5f98-4206-8247-370c1266a75f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "4595    0\n",
       "4596    0\n",
       "4597    0\n",
       "4598    0\n",
       "4599    0\n",
       "Name: is_spam, Length: 4600, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subset = X.iloc[:, :2]\n",
    "X_subset.describe()\n",
    "y_subset = y\n",
    "y_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data size  Training accuracy  Validation accuracy\n",
      "0       4370           0.518764             0.943478\n",
      "1       4370           0.616247             0.578261\n",
      "2        230           0.947826             0.905034\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "# Instantiate model LogisticRehression with maximum iteration of 2000\n",
    "#logisticRegression = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Create an empty DataFrame\n",
    "results = pd.DataFrame(columns=[\"Data size\", \"Training accuracy\", \"Validation accuracy\"])\n",
    "\n",
    "# List to store the sizes of the three training datasets\n",
    "data_sizes = []\n",
    "\n",
    "# List to store the training accuracies for the three datasets\n",
    "training_accuracies = []\n",
    "\n",
    "# List to store the validation accuracies for the three datasets\n",
    "validation_accuracies = []\n",
    "\n",
    "\n",
    "# implement the machine learning model with three different datasets\n",
    "\n",
    "# make X_subset the first two columns of X and y\n",
    "\n",
    "#X_subset = X.iloc[:, :2]\n",
    "#y_subset = y\n",
    "\n",
    "#Using the first dataset X and y instantiate a logisticRegression object\n",
    "logisticRegression_original = LogisticRegression(max_iter=2000) # create an instance of the logisticRegression model\n",
    "\n",
    "#split into training and test sets by given test size of 5%\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = train_test_split(X, y, test_size = 0.05, random_state = 0)\n",
    "\n",
    "data_sizes.append(len(X_train_original))\n",
    "\n",
    "#train the model\n",
    "logisticRegression_original.fit(X_train_original, y_train_original)\n",
    "\n",
    "# predict the value of y on the X_test set\n",
    "y_prediction_original_test = logisticRegression_original.predict(X_test_original)\n",
    "y_prediction_original_train = logisticRegression_original.predict(X_train_orginal)\n",
    "\n",
    "# Only the first two columns of X and y\n",
    "X_subset_train, X_subset_test, y_subset_train, y_subset_test = train_test_split(X_subset, y_subset, test_size = 0.05, random_state = 0)\n",
    "logisticRegression_subset = LogisticRegression(max_iter=2000)\n",
    "logisticRegression_subset.fit(X_subset_train, y_subset_train)\n",
    "\n",
    "data_sizes.append(len(X_subset_train))\n",
    "\n",
    "# prediction on the X_subset test size\n",
    "y_subset_column_prediction_test = logisticRegression_subset.predict(X_subset_test)\n",
    "y_subset_column_prediction_train = logisticRegression_subset.predict(X_subset_train)\n",
    "\n",
    "#implementation of the model on the X_small and y_small training set\n",
    "logisticRegression_small = LogisticRegression(max_iter = 2000)\n",
    "logisticRegression_small.fit(X_small_train, y_small_train)\n",
    "\n",
    "data_sizes.append(len(X_small_train))\n",
    "# prediction on the X_small training and X_small test set\n",
    "y_small_prediction_test = logisticRegression_small.predict(X_small_test)\n",
    "y_small_prediction_train = logisticRegression_small.predict(X_small_train)\n",
    "\n",
    "# To calculate the training and validation accuracy for the three different sets\n",
    "\n",
    "#Let us consider the original dataset X, y\n",
    "training_accuracies.append(accuracy_score(y_train_original, y_prediction_original_train))\n",
    "validation_accuracies.append(accuracy_score(y_test_original, y_prediction_original_test))\n",
    "\n",
    "\n",
    "# Let us consider the subset of 2 columns\n",
    "\n",
    "training_accuracies.append(accuracy_score(y_subset_train, y_subset_column_prediction_train))\n",
    "validation_accuracies.append(accuracy_score(y_subset_test, y_subset_column_prediction_test))\n",
    "\n",
    "\n",
    "#let us consider the X_small and y_small\n",
    "\n",
    "training_accuracies.append(accuracy_score(y_small_train, y_small_prediction_train))\n",
    "validation_accuracies.append(accuracy_score(y_small_test, y_small_prediction_test))\n",
    "\n",
    "# Add the results to the DataFrame\n",
    "results[\"Data size\"] = data_sizes\n",
    "results[\"Training accuracy\"] = training_accuracies\n",
    "results[\"Validation accuracy\"] = validation_accuracies\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "In this case, for dataset with small sample , there was a similarity between the validation accuracy and the training accuracy. The deviation from both values was about 0.03 but as the samples increased, the training acccuracy seem to dip significantly.\n",
    "It seems there must have been a lot of underfitting. There seems not to be a complex similarities between the features of the data. When unseen data was used during the validation, the accuracy seems to improve significantly.when I used more data at 95% for the training and 5% for testing but faltered when I used only 5% of the data for training and used the outcome on the test data at 95%.\n",
    "\n",
    "False positives occurs when the model predicts a positive output instead of the true output which is negative\n",
    "False negatives occurs when the model predicts a negative output when the true output should have been positive.\n",
    "The situation and reason for the analysis will determine with of the scenrio is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "I followed the algorithm as described in the lecture note provided in the classroom on D2L\n",
    "What I had to do was understand the data.I imported the data using the guide I got by learning the tutorial [here](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "Review the data : I understood the type of data and the column heads. I checked if there was any null function in any of the rows or columns.\n",
    "Since I could see the data was clean, I proceeded to divide the dataset into 3 according to the instruction. The orginal set with 5% test data and 95% train data, the second data was 5% train data and 95% test data while the \n",
    "third data was 2 column subset of the original data.\n",
    "I split the data using the train_test_split function from the sklearn model selection module library to get the split asa i  described it. The random size was left at 0 for consistency of the randomization.\n",
    "I imported the linear regression model from the sklearn module library and passed the training set I had split to it. I used the fit function to train the model and used the predict function to verify the learning model worked.\n",
    "Confirmation was done by calculating the validation and train accuracy of the data. This is compared to further look into the outcome for analysis.\n",
    "\n",
    "Challenges I faced includes understanding and revisiting some Numpy and panda codes for data retrieval and looping. I also had to use the scikit learn website a lot to understand and retrieve the codes to execute the different stages of the process.\n",
    "I needed to learn underfitting and overfitting to undertand the comparison and differences in the validation and training accuracy. Constant learning and practice is required to get good at machine learning. A deep understanding of the model itself will be necessary for success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1030, 8)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape of y: (1030,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets.loaders import load_concrete\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = load_concrete()\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cement         slag          ash        water       splast  \\\n",
      "count  1030.000000  1030.000000  1030.000000  1030.000000  1030.000000   \n",
      "mean    281.165631    73.895485    54.187136   181.566359     6.203112   \n",
      "std     104.507142    86.279104    63.996469    21.355567     5.973492   \n",
      "min     102.000000     0.000000     0.000000   121.750000     0.000000   \n",
      "25%     192.375000     0.000000     0.000000   164.900000     0.000000   \n",
      "50%     272.900000    22.000000     0.000000   185.000000     6.350000   \n",
      "75%     350.000000   142.950000   118.270000   192.000000    10.160000   \n",
      "max     540.000000   359.400000   200.100000   247.000000    32.200000   \n",
      "\n",
      "            coarse         fine          age  \n",
      "count  1030.000000  1030.000000  1030.000000  \n",
      "mean    972.918592   773.578883    45.662136  \n",
      "std      77.753818    80.175427    63.169912  \n",
      "min     801.000000   594.000000     1.000000  \n",
      "25%     932.000000   730.950000     7.000000  \n",
      "50%     968.000000   779.510000    28.000000  \n",
      "75%    1029.400000   824.000000    56.000000  \n",
      "max    1145.000000   992.600000   365.000000  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "#Get a summary of all the data in X\n",
    "K = X.describe()\n",
    "print(K)\n",
    "#check for missing values in feature matrix X\n",
    "missing_null_X = X.isnull()\n",
    "#print(missing_null_X)\n",
    "#check for missing values or null in the target vector y\n",
    "missing_null_y = y.isnull()\n",
    "#print(missing_null_y)\n",
    "#count the missing values in each column\n",
    "missing_count = missing_values.sum()\n",
    "#print(missing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "\n",
    "# Under consideration will be the full dataset, a small dataset which will be 5% data of the original dataset and a two column only of the original dataset\n",
    "# I will create a loop to execute this task for each of the dataset\n",
    "\n",
    "# create a dataset that only contains two columns of the original dataset\n",
    "\n",
    "X_TwoColumn = X.iloc[:, :2]\n",
    "\n",
    "# using train_test_split function, create the different subset of the original dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 0)\n",
    "\n",
    "# using train_test_split function to use just 5% of the original dataset\n",
    "X_5_percentData_train, X_5_percentData_test, y_5_percentData_train, y_5_percentData_test = train_test_split(X, y, test_size = 0.95, random_state = 0)\n",
    "\n",
    "X_TwoColumn_train, X_TwoColumn_test, y_TwoColumn_train, y_TwoColumn_test = train_test_split(X_TwoColumn, y, test_size = 0.05, random_state = 0)\n",
    "\n",
    "# Function to fit a model\n",
    "def fit_model(X_train, y_train):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Fit the model on the full dataset\n",
    "model_X = fit_model(X_train, y_train)\n",
    "\n",
    "# Fit the model on the 5% dataset\n",
    "model_5_percentData = fit_model(X_5_percentData_train, y_5_percentData_train)\n",
    "\n",
    "# Fit the model on the two-column dataset\n",
    "model_TwoColumn = fit_model(X_TwoColumn_train, y_TwoColumn_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "970c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE) measures the average squared difference between the actual values and the predicted values. \n",
    "# Lower MSE values indicate better model performance.\n",
    "\n",
    "# The R2 score quantifies the fraction of the variability in the target variable (dependent variable) that can be \n",
    "# anticipated based on the features (independent variables). It's a value between 0 and 1, \n",
    "# with 1 indicating that the model perfectly predicts the target variable and 0 indicating that the model doesn't explain any of the variance.\n",
    "\n",
    "# Create an empty DataFrame\n",
    "result = pd.DataFrame(columns=[\"MSE\", \"R2 Score\"])\n",
    "\n",
    "# Function to calculate MSE and R2 score\n",
    "def calculate_metrics(model, X_test, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    MSE = mean_squared_error(y_test, y_test_pred)\n",
    "    R2 = r2_score(y_test, y_test_pred)\n",
    "    return MSE, R2\n",
    "\n",
    "# calculate the MSE and R2 score for the full data test set\n",
    "MSE_X, R2_X = calculate_metrics(model_X, X_test, y_test)\n",
    "# calculate the MSE and R2 score for the full data train set\n",
    "MSE_X_train, R2_X_train = calculate_metrics(model_X, X_train, y_train)\n",
    "\n",
    "# Assign rows with the label to put data in the columns\n",
    "result.loc[\"Full_Data_test\"] = [MSE_X, R2_X]\n",
    "result.loc[\"Full Data_train\"] = [MSE_X_train, R2_X_train]\n",
    "\n",
    "# Calculate the MSE and R2 score for the 5% dataset on test data\n",
    "MSE_5_percentData_test, R2_5_percentData_test = calculate_metrics(model_5_percentData, X_5_percentData_test, y_5_percentData_test)\n",
    "# Calculate the MSE and R2 score for the 5% dataset on training data\n",
    "MSE_5_percentData_train, R2_5_percentData_train = calculate_metrics(model_5_percentData, X_5_percentData_train, y_5_percentData_train)\n",
    "\n",
    "result.loc[\"5% Data_test\"] = [MSE_5_percentData_test, R2_5_percentData_test]\n",
    "result.loc[\"5% Data_train\"] = [MSE_5_percentData_train, R2_5_percentData_train]\n",
    "\n",
    "\n",
    "# Calculate the MSE and R2 score for the Two column test dataset \n",
    "MSE_TwoColumn_test, R2_TwoColumn_test = calculate_metrics(model_TwoColumn, X_TwoColumn_test, y_TwoColumn_test)\n",
    "# Calculate the MSE and R2 score for the Two column dataset train data\n",
    "MSE_TwoColumn_train, R2_TwoColumn_train = calculate_metrics(model_TwoColumn, X_TwoColumn_train, y_TwoColumn_train)\n",
    "\n",
    "result.loc[\"Two-Column Data_test\"] = [MSE_TwoColumn_test, R2_TwoColumn_test]\n",
    "result.loc[\"Two-Column Data_train\"] = [MSE_TwoColumn_train, R2_TwoColumn_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Full_Data_test  Full Data_train  5% Data_test  5% Data_train  \\\n",
      "MSE           106.595189       107.313261    190.718486      90.347589   \n",
      "R2 Score        0.636482         0.613545      0.304147       0.755427   \n",
      "\n",
      "          Two-Column Data_test  Two-Column Data_train  \n",
      "MSE                 196.639050             186.938708  \n",
      "R2 Score              0.329408               0.326799  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# # Add the accuracy results to the results DataFrame\n",
    "# transpose the frame to match the request\n",
    "\n",
    "result = result.transpose()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b64da5-977f-4845-8712-ff7e9bb8719f",
   "metadata": {},
   "source": [
    "No. The linear regression model did not produce a better result than the logistic regression model. At all the datasets, the prediction seems to get worse on the actual data. There seems to be no clear pattern identification for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "I source my code at different state of the execution by using the [scikit learn website](https://www.scikit-yb.org/en/latest/tutorial.html#feature-extraction) and lecture notes examples. I also used [youtube video](https://www.youtube.com/watch?v=29ZQ3TDGgRQ).The same process as used in my earlier process was used here again. I imported the model and viewed the data to review and ensure there is no missing values. I used describe() to count each data entry is consistent for each column.I then proceeded to split and train the model. I had to read about the defination of mean squared error and R2 score. This helped me interprete the result of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "For the full data set, I used 20% to test the model. The result between the mean squared error and the R2 score for both the training and test did not seem to vary at all. But when compared to when less data was used, the model performed worse wehn trying to predict a pattern as seen in the R2 score between the training accuracy and the validation accuracy columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I liked that the process was defined step by step but it can be challenging trying to deal with missing data. Taking a decision about how to deal with missing data leaves a question about how that could have affected the outcome of the model. It can be a bit confusing trying to deal with how much of the data should be used for test and training the data. How can I know that the dataset used is not biased and free from noise. \n",
    "\n",
    "It is challenging going through the tutorial on scikit learn website if you are not mathematically inclined. The choice of what model to use has a foundation in mathematics and that is where the knowledge of mathematics will play a significant roll from the start when choosing what model to use for a specific type of datset.\n",
    "\n",
    "It is very motivating to see how the model predicts the pattern and gets the outcome right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge - Alpha: 0.001, MSE: 107.31326063548332, R2 Score: 0.6364816968566105\n",
      "Lasso - Alpha: 0.001, MSE: 107.31326069823315, R2 Score: 0.6364862921556714\n",
      "Ridge - Alpha: 0.01, MSE: 107.31326063548407, R2 Score: 0.6364817081342449\n",
      "Lasso - Alpha: 0.01, MSE: 107.3132669101057, R2 Score: 0.636527629748241\n",
      "Ridge - Alpha: 0.1, MSE: 107.31326063555903, R2 Score: 0.6364818209091729\n",
      "Lasso - Alpha: 0.1, MSE: 107.31388803830968, R2 Score: 0.6369382231639868\n",
      "Ridge - Alpha: 1.0, MSE: 107.31326064305226, R2 Score: 0.6364829485172998\n",
      "Lasso - Alpha: 1.0, MSE: 107.375946393138, R2 Score: 0.6407662627314561\n",
      "Ridge - Alpha: 10.0, MSE: 107.31326139115107, R2 Score: 0.6364942104990654\n",
      "Lasso - Alpha: 10.0, MSE: 109.13012460425415, R2 Score: 0.6544718019495821\n",
      "Ridge - Alpha: 100.0, MSE: 107.31333499468136, R2 Score: 0.6366054362624083\n",
      "Lasso - Alpha: 100.0, MSE: 148.77532864744654, R2 Score: 0.5172771180992553\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from yellowbrick.datasets.loaders import load_concrete\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = load_concrete()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.05, random_state = 0)\n",
    "\n",
    "# Create a list of alpha values on a logarithmic scale\n",
    "alphas = np.logspace(-3, 2, num=6)  # Values range from 0.001 to 100\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Create Ridge and Lasso models with the current alpha value\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "\n",
    "    # Fit the models to the training data\n",
    "    ridge_model.fit(X_train, y_train)  \n",
    "    lasso_model.fit(X_train, y_train) \n",
    "    \n",
    "    # Make predictions on validation data\n",
    "    ridge_predictions_train = ridge_model.predict(X_train)\n",
    "    lasso_predictions_train = lasso_model.predict(X_train)\n",
    "    ridge_predictions_test = ridge_model.predict(X_test)\n",
    "    lasso_predictions_test = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance (e.g., calculate MSE and R2 score)\n",
    "    ridge_mse = mean_squared_error(y_train, ridge_predictions_train)\n",
    "    ridge_r2 = r2_score(y_test, ridge_predictions_test)\n",
    "\n",
    "    lasso_mse = mean_squared_error(y_train, lasso_predictions_train)\n",
    "    lasso_r2 = r2_score(y_test, lasso_predictions_test)\n",
    "\n",
    "    print(f\"Ridge - Alpha: {alpha}, MSE: {ridge_mse}, R2 Score: {ridge_r2}\")\n",
    "    print(f\"Lasso - Alpha: {alpha}, MSE: {lasso_mse}, R2 Score: {lasso_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "Lasso with an alpha of 10 gave me the best R2 score at 0.65 on a full data set with 80% training data and 20% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f47052-eb87-45bd-a299-8ad186180cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
